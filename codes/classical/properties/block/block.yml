#######################################################
## This is a code entry in the error correction zoo. ##
##       https://github.com/errorcorrectionzoo       ##
#######################################################

code_id: block

name: 'Block code'

description: |
  A code intended to encode a piece, or block, of a data stream on a \textit{block} of \(n\) symbols.
  Each symbol is taken from some fixed possibly infinite alphabet \(\Sigma\) \cite[Ch. 3]{doi:10.1007/978-3-642-58575-3}, which can include bits, Galois fields, rings, or real numbers.

  The overall alphabet of the code is \(\Sigma^n\), and \(n\) is called the \textit{length} of the code.
  In some cases, only a subset of \(\Sigma^n\) is available to use for constructing the code.
  For example, in the case of spherical codes, one is constrained to \(n\)-dimensional real vectors on the unit sphere.

  An alternative more stringent definition (not used here) is in terms of a map encoding logical information from \(\Sigma^k\) into \(\Sigma^n\), yielding an \((n,k,d)_{\Sigma}\) block code, where \(d\) is the code distance.

# A common family of codes are the \textit{block codes}, intended to encode a piece, or block, of a data stream.
# A block code encodes strings of length \(k\), where each character in the string an element of some fixed alphabet \(\Sigma\), into strings of length \(n\). In other words, a block code encoding is a map from \(\Sigma^k\) to \(\Sigma^n\), where \(N = |\Sigma|^n\), \(K=|\Sigma|^k\), and \(|\Sigma|\) is the number of elements in the alphabet.

protection: |
  Block codes protect from errors acting on a few of the \(n\) symbols. A block code with \textit{distance} \(d\) detects errors acting on up to \(d-1\) symbols, and corrects erasure errors on up to \(d-1\) symbols.

features:
  rate: 'The Shannon channel capacity (the maximum of the mutual information over input and output distributions) is the highest rate of information transmission through a classical (i.e., non-quantum) channel with arbitrarily small error rate \cite{doi:10.1002/j.1538-7305.1948.tb01338.x}.
  Corrections to the capacity and tradeoff between decoding error, code rate and code length are determined using small \cite{manual:{V. Strassen, “Asymptotische Absch¨atzungen in Shannons Informationstheorie,” Trans. Third Prague Conference on Information Theory, Prague, 689–723, (1962)},arxiv:0801.2242,doi:10.1109/TIT.2010.2043769}, moderate \cite{arxiv:1208.1924,doi:10.1109/ALLERTON.2010.5707068,arxiv:1701.03114} and large \cite{doi:10.1007/978-3-7091-2945-6,doi:10.1017/CBO9780511921889,doi:10.1109/TIT.1973.1055007,doi:10.1109/TIT.1979.1056003} deviation analysis. Sometimes the difference from the asymptotic rate at finite block length can be characterized by the \textit{channel dispersion} \cite{doi:10.1109/TIT.2010.2043769,doi:10.1109/TIT.2014.2341919}.'
  decoders:
    - 'Decoding an error-correcting code is equivalent to finding the ground state of some statistical mechanical model \cite{doi:10.1038/339693a0}.'


relations:
  parents:
    - code_id: ecc
  cousins:
    - code_id: block_quantum


# Begin Entry Meta Information
_meta:
  # Change log - most recent first
  changelog:
    - user_id: VictorVAlbert
      date: '2023-02-14'
